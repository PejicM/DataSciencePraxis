{"cells":[{"cell_type":"markdown","source":["# Custom Evaluators\n\nLet's create the Mean Absolute Error evaluator from scratch:"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import * \nfrom pyspark.ml.evaluation import Evaluator\nfrom pyspark.ml.param import Param, Params, TypeConverters\nfrom pyspark.ml.param.shared import HasLabelCol, HasPredictionCol, HasRawPredictionCol, \\\n    HasFeaturesCol\n  \nclass MAEEvaluator(Evaluator, HasLabelCol, HasPredictionCol):\n  def _evaluate(self, dataset):\n    \"\"\"\n    Evaluates the output.\n\n    :param dataset: a dataset that contains labels/observations and predictions\n    :return: metric\n    \"\"\"\n    labelCol = self.getLabelCol()\n    predictionCol = self.getPredictionCol()\n    return dataset.select(avg(abs(col(predictionCol)-col(labelCol)))).first()[0]\n\n  def isLargerBetter(self):\n    \"\"\"\n    Indicates whether the metric returned should be maximized\n    (True, default) or minimized (False).\n    A given evaluator may support multiple metrics which may be maximized or minimized.\n    \"\"\"\n    return False"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":["Now create and train a model:"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml import Pipeline\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.regression import LinearRegression, LinearRegressionModel\n\nbaseballDF = (spark.read\n            .option(\"header\", True)\n            .option(\"inferSchema\", True)\n            .csv(\"dbfs:/mnt/training/301/batting.csv\"))\n\n(testDF, trainingDF) = baseballDF.select(\"r\",\"h\",\"double\",\"triple\",\"hr\",\"rbi\", \"bb\").na.drop().randomSplit((0.20, 0.80), seed=42)\ntestDF.cache()\ntrainingDF.cache()\n\nvecAssembler = VectorAssembler()\nvecAssembler.setInputCols([\"h\", \"double\", \"triple\", \"hr\", \"rbi\", \"bb\"])\nvecAssembler.setOutputCol(\"features\")\n\nlr = LinearRegression()\nlr.setLabelCol(\"r\")\n\nlrPipeline = Pipeline()\nlrPipeline.setStages([vecAssembler, lr])\nlrPipelineModel = lrPipeline.fit(trainingDF)\n\npredictionsDF = lrPipelineModel.transform(testDF)"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["...and evaluate the results:"],"metadata":{}},{"cell_type":"code","source":["eval = MAEEvaluator()\neval.setLabelCol(\"r\")\n\n#dir(eval)\neval.evaluate(predictionsDF)"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":7}],"metadata":{"name":"Custom Evaluators","notebookId":417702524457858},"nbformat":4,"nbformat_minor":0}
